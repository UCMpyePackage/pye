% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mmAPG_and_mnmAPG_algorithms.R
\name{mmAPG}
\alias{mmAPG}
\title{mmAPG}
\usage{
mmAPG(
  x0,
  c_pos = length(x0),
  delta_fx,
  proxx,
  Fx,
  lambda = NULL,
  penalty = NULL,
  fold = NULL,
  stepsizeShrink = 0.8,
  max_alpha = 10000,
  min_alpha = 1e-10,
  delta = 1e-05,
  trace = 2,
  seed = 1,
  max_iter = 10000,
  convergence_error = 1e-07,
  zeros_stay_zeros_from_iteration = 20,
  max.print = 10
)
}
\arguments{
\item{x0}{starting point (to encourage a fast convergence to a sparse result
we suggest to use the zero vector as starting point)}

\item{c_pos}{position of the constant term. Equal it to NULL if you don't use
any constant.}

\item{delta_fx}{gradient of fx}

\item{proxx}{proximal operator related to gx}

\item{Fx}{fx + gx}

\item{lambda}{the penalization parameter of the regressors X, related to gx,
just for the visualization in the trace report. Default is NULL}

\item{penalty}{penalty parameter, related to gx, just for the visualization
in the trace report. Default is NULL}

\item{fold}{fold in which you are doing the cross-validation, just for the
visualization in the trace report. Default is NULL}

\item{stepsizeShrink}{parameter to adjust the step-size in the backtracking
line-search, in the optimization of pye. Taking values between 0 and 1,
the closer to 1, the more accurate the estimation will be, the longer it
will take and viceversa. Default is 0.8}

\item{max_alpha}{maximum value of the step-parameter alpha. Default is 1000}

\item{min_alpha}{minimum value of the step-parameter alpha. Default is 1e-12}

\item{delta}{parameter for the convergence condition of the optimization
algorithm. Default is 1e-5}

\item{trace}{2:visualize all the steps, 1:visualize just the result,
0:visualize nothing. Default is 2}

\item{seed}{fix the seed. Default is 1}

\item{max_iter}{maximum number of iterations in the algorithms mmAPG and
mnmAPG. Default is 500}

\item{convergence_error}{error to accept for considering the algorithm
converged. Default is 1e-5}

\item{zeros_stay_zeros_from_iteration}{the number of iteration from which
each parameter that reached zero in the estimation cannot change anymore.
This is done to preserve  the sparsity of the solution. Default is 5}

\item{max.print}{number of elements to show if printing the results. Default
is 10}
}
\value{
a list with x1 the estimated vector of parameters, tot_iters the
total number of iterations and backtrack_iters the number of backtracking
iterations.
}
\description{
Inspired by the paper Li and Lin, 2015, "Accelerated Proximal
Gradient Methods for Nonconvex Programming", this is the monotonone
optimization algorithm used in PYE. It is a modified version of the original
of Li and Lin: Changes have been proposed in the initialization of the
algorithm and in the selection part. With respect of this second point:
we did not give the possibility to come back to deleted variables, i.e. the
selection is not reversible
}
\examples{
library(pye)
cols <- 2000
cols_cov <- 20
seed=1
simMicroarrayData_cov02_dim50_covariates <- create_sample_with_covariates(
		rows_train=50, cols=cols, cols_cov=cols_cov, covar=0.2, seed=seed)
df <- simMicroarrayData_cov02_dim50_covariates$train_df_scaled
X <- simMicroarrayData_cov02_dim50_covariates$X
y <- simMicroarrayData_cov02_dim50_covariates$y
ID <- rownames(df)
df1 <- cbind(ID, df[,(names(df) \%in\% c(y,X))])
penalty <- "L12"
c_zero_fixed <- TRUE
lambda <- 0.1
max_iter <- 10
kernel <- "gaussian"
if (penalty == "SCAD") {a=3.7} else {a=3.0}
prox_penalty <- get(paste0("proximal_operator_", penalty))

#wrappers
delta_fx <- function(x){
 if (c_zero_fixed==TRUE){
   x[names(x) == "c"]<-0
  }
  result <- -pye_KS(df=df1[,names(df1)!="ID"],, X=X, y=y,
    betas=x[!(names(x) == "c")], lambda=lambda, c=x[(names(x) == "c")],
    kernel=kernel, alpha=0.5, a1=3.7, a2=3, penalty=penalty)$gr_yi
  if (c_zero_fixed==TRUE){
    result[names(result) == "c"] <- 0
  }
  return(result)
}

proxx <- function(x, eta){
 if (c_zero_fixed==TRUE){
    x[names(x) == "c"]<-0
  }
  result <- c(prox_penalty(betas=x[!(names(x) == "c")], lambda=eta*lambda,
    alpha=0.5, a=a), x[(names(x) == "c")])
  return(result)
}

Fx <- function(x){
  if (c_zero_fixed==TRUE){
    x[(names(x) == "c")]<-0
  }
  result <- -getElement(pye_KS(df=df1[,names(df1)!="ID"], X=X, y=y,
    betas=x[!(names(x) == "c")], lambda=lambda, c=x[(names(x) == "c")],
    kernel=kernel, alpha=0.5, a1=3.7, a2=3, penalty=penalty),
    paste0("pye_", penalty))
  return(result)
}

#starting point:
x0 <- c(0, rep(0, length(X))) #the first zero is the constant term
names(x0) <- c("c",X)

estim <- mmAPG(x0=x0, delta_fx=delta_fx, proxx=proxx, Fx=Fx, lambda=lambda,
penalty=penalty, max_iter=max_iter, trace=2)
print(estim)
}
